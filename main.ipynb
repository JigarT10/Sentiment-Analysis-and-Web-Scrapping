{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "0fc4e768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\91942\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n",
      "105\n",
      "110\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('cmudict')\n",
    "cmudict = nltk.corpus.cmudict.dict()\n",
    "\n",
    "list_positive_scores = []\n",
    "list_negative_scores = []\n",
    "list_polarity_scores = []\n",
    "list_subjectivity_scores = []\n",
    "list_avg_sentence_length = []\n",
    "list_percentage_of_complex_words = []\n",
    "list_fog_index = []\n",
    "list_avg_words_per_sentence = []\n",
    "list_total_complex_words_count = []\n",
    "list_total_words_count = []\n",
    "list_syllable_per_word = []\n",
    "list_personal_pronounce = []\n",
    "list_avg_word_length = []\n",
    "\n",
    "\n",
    "# Remove punctuation marks from the paragraph\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# STOPWORDS\n",
    "# Add the local path to your folder\n",
    "stopwords_path = 'C:/Users/91942/Desktop/Blackcoffer/Data/StopWords'\n",
    "\n",
    "# get a list of all text files in the folder\n",
    "stopwords_files_list = glob.glob(os.path.join(stopwords_path, '*.txt'))\n",
    "\n",
    "# loop through each file and read its contents\n",
    "allstopwords = ''\n",
    "for stopwords_file in stopwords_files_list:\n",
    "    with open(stopwords_file, 'r') as stopwords:\n",
    "        allstopwords = allstopwords + ' ' + stopwords.read()\n",
    "        # do something with the file contents\n",
    "        \n",
    "# Remove punctuation marks from the allstopwords\n",
    "allstopwords = allstopwords.translate(translator)\n",
    "\n",
    "allstopwords = allstopwords.lower()\n",
    "allstopwords = allstopwords.replace(\"\\n\",\" \")\n",
    "allstopwords = allstopwords.split(\" \")\n",
    "while \" \" in allstopwords:\n",
    "    allstopwords.remove('')\n",
    "\n",
    "# The Master Dictionary (found in the folder MasterDictionary) is used for creating a dictionary of Positive and Negative words.\n",
    "with open('C:/Users/91942/Desktop/Blackcoffer/Data/MasterDictionary/positive-words.txt', 'r') as file:\n",
    "    positive_words = file.read()\n",
    "positive_words = positive_words.split('\\n')\n",
    "\n",
    "# The Master Dictionary (found in the folder MasterDictionary) is used for creating a dictionary of Positive and Negative words.\n",
    "with open('C:/Users/91942/Desktop/Blackcoffer/Data/MasterDictionary/negative-words.txt', 'r') as file:\n",
    "    negative_words = file.read()\n",
    "negative_words = negative_words.split('\\n')\n",
    "\n",
    "vowels = ['a', 'e', 'i', 'o' ,'u']\n",
    "\n",
    "input = pd.read_excel(\"Data/input.xlsx\")\n",
    "\n",
    "for index in range(114):\n",
    "    page = ''\n",
    "    page = requests.get(input.iloc[index][1]).text\n",
    "    \n",
    "    soup = BeautifulSoup(page,'lxml')\n",
    "    \n",
    "    title = soup.find('title').text\n",
    "    \n",
    "    div = soup.find_all('div', class_='td-post-content')\n",
    "    \n",
    "    ptags = 0\n",
    "    if len(div) != 0: # index 7\n",
    "        ptags = len(div[0].find_all('p'))\n",
    "    \n",
    "    art_text = ''\n",
    "    for j in range(ptags):\n",
    "        art_text = art_text + ' ' + div[0].find_all('p')[j].text\n",
    "        \n",
    "    article = title + art_text\n",
    "    \n",
    "    article = article.replace('₹', 'Rs.') # index 47\n",
    "    article = article.replace('≈', '~') # index 80\n",
    "    article = article.replace(\"\\xa0\",\"\")\n",
    "    article = article.replace(\"’\",\"'\")\n",
    "    \n",
    "    filename = input.iloc[index][1].replace(':','%3A')\n",
    "    filename = filename.replace('/','%2F')\n",
    "    filename = filename + '.txt'\n",
    "    \n",
    "#     f = open(filename,\"x\")\n",
    "#     f.write(article)\n",
    "#     f.close()\n",
    "    \n",
    "    article_lower = article.lower()\n",
    "    \n",
    "    # Remove punctuation marks from the paragraph\n",
    "    clean_article = article_lower.translate(translator)\n",
    "\n",
    "    clean_article_list = clean_article.split(' ')\n",
    "    while '' in clean_article_list:\n",
    "        clean_article_list.remove('')\n",
    "        \n",
    "    '''\n",
    "    The Stop Words Lists (found in the folder StopWords) are used to clean the text\n",
    "    so that Sentiment Analysis can be performed by excluding the words found in Stop Words List.\n",
    "    '''\n",
    "    j = -1\n",
    "    no_stopwords_list = clean_article_list.copy()\n",
    "    for i in range(len(no_stopwords_list)):\n",
    "        if no_stopwords_list[j] in allstopwords:\n",
    "            no_stopwords_list.pop(j)\n",
    "        else:\n",
    "            j = j -1\n",
    "    \n",
    "    # Positive Score:\n",
    "    #    This score is calculated by assigning the value of +1 for each word if found in the Positive Dictionary and then adding up all the values.\n",
    "    positive_score = 0\n",
    "    for i in no_stopwords_list:\n",
    "        if i in positive_words:\n",
    "            positive_score += 1\n",
    "    list_positive_scores.append(positive_score)\n",
    "\n",
    "    # Negative Score:\n",
    "    #    This score is calculated by assigning the value of +1 for each word if found in the Negative Dictionary and then adding up all the values.\n",
    "    negative_score = 0\n",
    "    for i in no_stopwords_list:\n",
    "        if i in negative_words:\n",
    "            negative_score += 1\n",
    "    list_negative_scores.append(negative_score)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Polarity Score: This is the score that determines if a given text is positive or negative in nature.\n",
    "                    It is calculated by using the formula: \n",
    "    Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n",
    "                    Range is from -1 to +1\n",
    "\n",
    "    '''\n",
    "    polarity_score = (positive_score - negative_score)/ ((positive_score + negative_score) + 0.000001)\n",
    "    list_polarity_scores.append(polarity_score)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Subjectivity Score: This is the score that determines if a given text is objective or subjective.\n",
    "                        It is calculated by using the formula: \n",
    "    Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n",
    "                        Range is from 0 to +1\n",
    "\n",
    "    '''\n",
    "    subjectivity_score = (positive_score + negative_score)/ (len(no_stopwords_list) + 0.000001)\n",
    "    list_subjectivity_scores.append(subjectivity_score)\n",
    "        \n",
    "    sentences  = nltk.sent_tokenize(article)\n",
    "    \n",
    "    words = len(clean_article_list)\n",
    "    list_total_words_count.append(words)\n",
    "    \n",
    "    avg_words_per_sentence = words / len(sentences)\n",
    "    # Average Number of Words Per Sentence = the total number of words / the total number of sentences\n",
    "    list_avg_words_per_sentence.append(avg_words_per_sentence)\n",
    "    # Average Sentence Length = the number of words / the number of sentences\n",
    "    list_avg_sentence_length.append(avg_words_per_sentence)\n",
    "    \n",
    "    # Complex words are words in the text that contain more than two syllables.\n",
    "    complex_words = 0\n",
    "    total_syllables = 0\n",
    "    syllables = 0\n",
    "\n",
    "    esed = ['es', 'ed']\n",
    "    for i in clean_article_list:\n",
    "        syllables = 0\n",
    "        \n",
    "        if i[-2:] == 'es' or i[-2:] == 'ed': i = i[:-2]\n",
    "            \n",
    "        for j in i:\n",
    "            if j in vowels: syllables += 1\n",
    "        \n",
    "        if syllables > 2: complex_words += 1\n",
    "            \n",
    "        total_syllables += syllables\n",
    "                \n",
    "    list_total_complex_words_count.append(complex_words)\n",
    "\n",
    "    \n",
    "    syllables_per_word = total_syllables / len(clean_article_list)\n",
    "    list_syllable_per_word.append(syllables_per_word)\n",
    "    \n",
    "    # Percentage of Complex words = the number of complex words / the number of words \n",
    "    percent_of_complex_words = complex_words / len(clean_article_list)\n",
    "    list_percentage_of_complex_words.append(percent_of_complex_words)\n",
    "    \n",
    "    # Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "    fog_index = 0.4 * (avg_words_per_sentence + percent_of_complex_words)\n",
    "    list_fog_index.append(fog_index)\n",
    "    \n",
    "    # Define the regular expression pattern to match personal pronouns\n",
    "    pattern = r'\\b(I|me|my|mine|we|us|our|ours|Me|My|Mine|We|Us|Our|Ours)\\b'\n",
    "\n",
    "    # Find all matches of the pattern in the article\n",
    "    personal_pronouns_list = re.findall(pattern, article)\n",
    "    personal_pronouns_count = len(personal_pronouns_list)\n",
    "    list_personal_pronounce.append(personal_pronouns_count)\n",
    "    \n",
    "    char_count = 0\n",
    "    for i in clean_article_list:\n",
    "        for j in i:\n",
    "            char_count += 1\n",
    "    list_avg_word_length.append(char_count / words)\n",
    "    \n",
    "    if index % 5 == 0:\n",
    "        print(index)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'POSITIVE SCORE': list_positive_scores,\n",
    "                   'NEGATIVE SCORE': list_negative_scores,\n",
    "                   'POLARITY SCORE': list_polarity_scores,\n",
    "                   'SUBJECTIVITY SCORE': list_subjectivity_scores,\n",
    "                   'AVG SENTENCE LENGTH': list_avg_sentence_length,\n",
    "                   'PERCENTAGE OF COMPLEX WORDS': list_percentage_of_complex_words,\n",
    "                   'FOG INDEX': list_fog_index,\n",
    "                   'AVG NUMBER OF WORDS PER SENTENCE': list_avg_words_per_sentence,\n",
    "                   'COMPLEX WORD COUNT': list_total_complex_words_count,\n",
    "                   'WORD COUNT': list_total_words_count,\n",
    "                   'SYLLABLE PER WORD': list_syllable_per_word,\n",
    "                   'PERSONAL PRONOUNS': list_personal_pronounce,\n",
    "                   'AVG WORD LENGTH': list_avg_word_length})\n",
    "\n",
    "output = pd.concat([input, df], axis=1)\n",
    "output.to_csv('C:/Users/91942/Desktop/Blackcoffer/Data/Output Data Structure.csv')\n",
    "output.to_excel('C:/Users/91942/Desktop/Blackcoffer/Data/Output Data Structure.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "16a3f2d4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62</td>\n",
       "      <td>32</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>0.098739</td>\n",
       "      <td>23.760000</td>\n",
       "      <td>0.219978</td>\n",
       "      <td>9.591991</td>\n",
       "      <td>23.760000</td>\n",
       "      <td>392</td>\n",
       "      <td>1782</td>\n",
       "      <td>1.775533</td>\n",
       "      <td>4</td>\n",
       "      <td>5.670595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>37</td>\n",
       "      <td>0.221053</td>\n",
       "      <td>0.174312</td>\n",
       "      <td>17.625000</td>\n",
       "      <td>0.145390</td>\n",
       "      <td>7.108156</td>\n",
       "      <td>17.625000</td>\n",
       "      <td>205</td>\n",
       "      <td>1410</td>\n",
       "      <td>1.560993</td>\n",
       "      <td>8</td>\n",
       "      <td>4.842553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>35</td>\n",
       "      <td>0.292929</td>\n",
       "      <td>0.123441</td>\n",
       "      <td>19.952941</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>8.064195</td>\n",
       "      <td>19.952941</td>\n",
       "      <td>352</td>\n",
       "      <td>1696</td>\n",
       "      <td>1.761203</td>\n",
       "      <td>3</td>\n",
       "      <td>5.400943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59</td>\n",
       "      <td>27</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.140753</td>\n",
       "      <td>17.391304</td>\n",
       "      <td>0.151250</td>\n",
       "      <td>7.017022</td>\n",
       "      <td>17.391304</td>\n",
       "      <td>242</td>\n",
       "      <td>1600</td>\n",
       "      <td>1.593750</td>\n",
       "      <td>22</td>\n",
       "      <td>4.805625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53</td>\n",
       "      <td>23</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.102426</td>\n",
       "      <td>22.077922</td>\n",
       "      <td>0.171176</td>\n",
       "      <td>8.899639</td>\n",
       "      <td>22.077922</td>\n",
       "      <td>291</td>\n",
       "      <td>1700</td>\n",
       "      <td>1.604706</td>\n",
       "      <td>20</td>\n",
       "      <td>5.078824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>41</td>\n",
       "      <td>22</td>\n",
       "      <td>0.301587</td>\n",
       "      <td>0.121622</td>\n",
       "      <td>22.527273</td>\n",
       "      <td>0.155771</td>\n",
       "      <td>9.073217</td>\n",
       "      <td>22.527273</td>\n",
       "      <td>193</td>\n",
       "      <td>1239</td>\n",
       "      <td>1.597256</td>\n",
       "      <td>36</td>\n",
       "      <td>5.003228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.100610</td>\n",
       "      <td>16.377778</td>\n",
       "      <td>0.166893</td>\n",
       "      <td>6.617868</td>\n",
       "      <td>16.377778</td>\n",
       "      <td>123</td>\n",
       "      <td>737</td>\n",
       "      <td>1.620081</td>\n",
       "      <td>11</td>\n",
       "      <td>5.065129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>6.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>19.638889</td>\n",
       "      <td>0.117397</td>\n",
       "      <td>7.902515</td>\n",
       "      <td>19.638889</td>\n",
       "      <td>83</td>\n",
       "      <td>707</td>\n",
       "      <td>1.463932</td>\n",
       "      <td>3</td>\n",
       "      <td>4.830269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>61</td>\n",
       "      <td>33</td>\n",
       "      <td>0.297872</td>\n",
       "      <td>0.103638</td>\n",
       "      <td>26.604938</td>\n",
       "      <td>0.169838</td>\n",
       "      <td>10.709910</td>\n",
       "      <td>26.604938</td>\n",
       "      <td>366</td>\n",
       "      <td>2155</td>\n",
       "      <td>1.587007</td>\n",
       "      <td>12</td>\n",
       "      <td>4.939675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>42</td>\n",
       "      <td>54</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.110599</td>\n",
       "      <td>20.054348</td>\n",
       "      <td>0.152304</td>\n",
       "      <td>8.082661</td>\n",
       "      <td>20.054348</td>\n",
       "      <td>281</td>\n",
       "      <td>1845</td>\n",
       "      <td>1.507317</td>\n",
       "      <td>1</td>\n",
       "      <td>5.151762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0               62              32        0.319149            0.098739   \n",
       "1               58              37        0.221053            0.174312   \n",
       "2               64              35        0.292929            0.123441   \n",
       "3               59              27        0.372093            0.140753   \n",
       "4               53              23        0.394737            0.102426   \n",
       "5               41              22        0.301587            0.121622   \n",
       "6               22              11        0.333333            0.100610   \n",
       "7                0               0        0.000000            0.000000   \n",
       "8               32              13        0.422222            0.150000   \n",
       "9               61              33        0.297872            0.103638   \n",
       "10              42              54       -0.125000            0.110599   \n",
       "\n",
       "    AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0             23.760000                     0.219978   9.591991   \n",
       "1             17.625000                     0.145390   7.108156   \n",
       "2             19.952941                     0.207547   8.064195   \n",
       "3             17.391304                     0.151250   7.017022   \n",
       "4             22.077922                     0.171176   8.899639   \n",
       "5             22.527273                     0.155771   9.073217   \n",
       "6             16.377778                     0.166893   6.617868   \n",
       "7              5.000000                     0.000000   2.000000   \n",
       "8             19.638889                     0.117397   7.902515   \n",
       "9             26.604938                     0.169838  10.709910   \n",
       "10            20.054348                     0.152304   8.082661   \n",
       "\n",
       "    AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                          23.760000                 392        1782   \n",
       "1                          17.625000                 205        1410   \n",
       "2                          19.952941                 352        1696   \n",
       "3                          17.391304                 242        1600   \n",
       "4                          22.077922                 291        1700   \n",
       "5                          22.527273                 193        1239   \n",
       "6                          16.377778                 123         737   \n",
       "7                           5.000000                   0           5   \n",
       "8                          19.638889                  83         707   \n",
       "9                          26.604938                 366        2155   \n",
       "10                         20.054348                 281        1845   \n",
       "\n",
       "    SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0            1.775533                  4         5.670595  \n",
       "1            1.560993                  8         4.842553  \n",
       "2            1.761203                  3         5.400943  \n",
       "3            1.593750                 22         4.805625  \n",
       "4            1.604706                 20         5.078824  \n",
       "5            1.597256                 36         5.003228  \n",
       "6            1.620081                 11         5.065129  \n",
       "7            1.000000                  0         6.200000  \n",
       "8            1.463932                  3         4.830269  \n",
       "9            1.587007                 12         4.939675  \n",
       "10           1.507317                  1         5.151762  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'POSITIVE SCORE': list_positive_scores,\n",
    "                   'NEGATIVE SCORE': list_negative_scores,\n",
    "                   'POLARITY SCORE': list_polarity_scores,\n",
    "                   'SUBJECTIVITY SCORE': list_subjectivity_scores,\n",
    "                   'AVG SENTENCE LENGTH': list_avg_sentence_length,\n",
    "                   'PERCENTAGE OF COMPLEX WORDS': list_percentage_of_complex_words,\n",
    "                   'FOG INDEX': list_fog_index,\n",
    "                   'AVG NUMBER OF WORDS PER SENTENCE': list_avg_words_per_sentence,\n",
    "                   'COMPLEX WORD COUNT': list_total_complex_words_count,\n",
    "                   'WORD COUNT': list_total_words_count,\n",
    "                   'SYLLABLE PER WORD': list_syllable_per_word,\n",
    "                   'PERSONAL PRONOUNS': list_personal_pronounce,\n",
    "                   'AVG WORD LENGTH': list_avg_word_length})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "24e1f351",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['challenges',\n",
       " 'and',\n",
       " 'opportunities',\n",
       " 'of',\n",
       " 'big',\n",
       " 'data',\n",
       " 'in',\n",
       " 'healthcare',\n",
       " 'blackcoffer',\n",
       " 'insights',\n",
       " 'to',\n",
       " 'begin',\n",
       " 'with',\n",
       " 'i',\n",
       " 'shall',\n",
       " 'first',\n",
       " 'like',\n",
       " 'to',\n",
       " 'explain',\n",
       " 'what',\n",
       " 'big',\n",
       " 'data',\n",
       " 'is',\n",
       " 'and',\n",
       " 'why',\n",
       " 'it',\n",
       " 'has',\n",
       " 'become',\n",
       " 'so',\n",
       " 'important',\n",
       " 'in',\n",
       " 'our',\n",
       " 'lives',\n",
       " 'big',\n",
       " 'data',\n",
       " 'is',\n",
       " 'simply',\n",
       " 'data',\n",
       " 'but',\n",
       " 'with',\n",
       " 'a',\n",
       " 'huge',\n",
       " 'size',\n",
       " 'data',\n",
       " 'that',\n",
       " 'is',\n",
       " 'not',\n",
       " 'just',\n",
       " 'voluminous',\n",
       " 'but',\n",
       " 'also',\n",
       " 'growing',\n",
       " 'exponentially',\n",
       " 'with',\n",
       " 'time',\n",
       " 'such',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'are',\n",
       " 'so',\n",
       " 'large',\n",
       " 'and',\n",
       " 'complex',\n",
       " 'that',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'possible',\n",
       " 'to',\n",
       " 'store',\n",
       " 'or',\n",
       " 'process',\n",
       " 'them',\n",
       " 'using',\n",
       " 'traditional',\n",
       " 'data',\n",
       " 'management',\n",
       " 'tools',\n",
       " 'so',\n",
       " 'how',\n",
       " 'huge',\n",
       " 'can',\n",
       " 'this',\n",
       " 'data',\n",
       " 'be',\n",
       " 'social',\n",
       " 'media',\n",
       " 'sites',\n",
       " 'like',\n",
       " 'facebook',\n",
       " 'generate',\n",
       " 'more',\n",
       " 'than',\n",
       " '500',\n",
       " 'terabytes',\n",
       " 'of',\n",
       " 'new',\n",
       " 'data',\n",
       " 'every',\n",
       " 'day',\n",
       " 'in',\n",
       " 'the',\n",
       " 'form',\n",
       " 'of',\n",
       " 'photos',\n",
       " 'video',\n",
       " 'uploads',\n",
       " 'text',\n",
       " 'messages',\n",
       " 'etc',\n",
       " 'a',\n",
       " 'singlejet',\n",
       " 'enginecan',\n",
       " 'generatemore',\n",
       " 'than',\n",
       " '10',\n",
       " 'terabytesof',\n",
       " 'data',\n",
       " 'in30',\n",
       " 'minutesof',\n",
       " 'flight',\n",
       " 'time',\n",
       " 'with',\n",
       " 'many',\n",
       " 'thousand',\n",
       " 'flights',\n",
       " 'per',\n",
       " 'day',\n",
       " 'generation',\n",
       " 'of',\n",
       " 'data',\n",
       " 'reaches',\n",
       " 'up',\n",
       " 'to',\n",
       " 'manypetabytes',\n",
       " 'data',\n",
       " 'contained',\n",
       " 'in',\n",
       " 'these',\n",
       " 'sets',\n",
       " 'are',\n",
       " 'not',\n",
       " 'always',\n",
       " 'structured',\n",
       " 'it',\n",
       " 'can',\n",
       " 'be',\n",
       " 'semistructured',\n",
       " 'or',\n",
       " 'even',\n",
       " 'unstructured',\n",
       " 'when',\n",
       " 'such',\n",
       " 'is',\n",
       " 'the',\n",
       " 'size',\n",
       " 'and',\n",
       " 'dimension',\n",
       " 'of',\n",
       " 'data',\n",
       " 'we',\n",
       " 'can',\n",
       " 'well',\n",
       " 'imagine',\n",
       " 'how',\n",
       " 'complicated',\n",
       " 'it',\n",
       " 'must',\n",
       " 'be',\n",
       " 'to',\n",
       " 'process',\n",
       " 'and',\n",
       " 'analyze',\n",
       " 'it',\n",
       " 'therefore',\n",
       " 'modern',\n",
       " 'methods',\n",
       " 'of',\n",
       " 'data',\n",
       " 'analysis',\n",
       " 'are',\n",
       " 'being',\n",
       " 'developed',\n",
       " 'and',\n",
       " 'used',\n",
       " 'to',\n",
       " 'process',\n",
       " 'the',\n",
       " 'information',\n",
       " 'contained',\n",
       " 'in',\n",
       " 'big',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'now',\n",
       " 'coming',\n",
       " 'to',\n",
       " 'the',\n",
       " 'opportunities',\n",
       " 'that',\n",
       " 'big',\n",
       " 'data',\n",
       " 'provides',\n",
       " 'they',\n",
       " 'are',\n",
       " 'not',\n",
       " 'just',\n",
       " 'limited',\n",
       " 'to',\n",
       " 'healthcare',\n",
       " 'big',\n",
       " 'data',\n",
       " 'analysis',\n",
       " 'is',\n",
       " 'now',\n",
       " 'a',\n",
       " 'disruptive',\n",
       " 'technology',\n",
       " 'which',\n",
       " 'has',\n",
       " 'intervened',\n",
       " 'into',\n",
       " 'numerous',\n",
       " 'fields',\n",
       " 'and',\n",
       " 'proved',\n",
       " 'its',\n",
       " 'worth',\n",
       " 'healthcare',\n",
       " 'is',\n",
       " 'no',\n",
       " 'exception',\n",
       " 'big',\n",
       " 'data',\n",
       " 'has',\n",
       " 'the',\n",
       " 'potential',\n",
       " 'to',\n",
       " 'change',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'dynamics',\n",
       " 'of',\n",
       " 'the',\n",
       " 'healthcare',\n",
       " 'industry',\n",
       " 'and',\n",
       " 'improve',\n",
       " 'the',\n",
       " 'quality',\n",
       " 'of',\n",
       " 'life',\n",
       " 'of',\n",
       " 'people',\n",
       " 'healthcare',\n",
       " 'industry',\n",
       " 'is',\n",
       " 'a',\n",
       " 'very',\n",
       " 'large',\n",
       " 'and',\n",
       " 'complicated',\n",
       " 'system',\n",
       " 'it',\n",
       " 'involves',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'risks',\n",
       " 'and',\n",
       " 'always',\n",
       " 'demands',\n",
       " 'better',\n",
       " 'care',\n",
       " 'however',\n",
       " 'when',\n",
       " 'a',\n",
       " 'large',\n",
       " 'number',\n",
       " 'of',\n",
       " 'patients',\n",
       " 'seek',\n",
       " 'emergency',\n",
       " 'care',\n",
       " 'the',\n",
       " 'complications',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'the',\n",
       " 'cost',\n",
       " 'rise',\n",
       " 'exponentially',\n",
       " 'in',\n",
       " 'india',\n",
       " 'many',\n",
       " 'times',\n",
       " 'we',\n",
       " 'even',\n",
       " 'lack',\n",
       " 'sufficient',\n",
       " 'infrastructure',\n",
       " 'to',\n",
       " 'support',\n",
       " 'the',\n",
       " 'patients',\n",
       " 'there',\n",
       " 'is',\n",
       " 'a',\n",
       " 'deficit',\n",
       " 'of',\n",
       " 'beds',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'doctors',\n",
       " 'to',\n",
       " 'provide',\n",
       " 'treatment',\n",
       " 'to',\n",
       " 'the',\n",
       " 'patients',\n",
       " 'for',\n",
       " 'instance',\n",
       " 'when',\n",
       " 'epidemics',\n",
       " 'break',\n",
       " 'out',\n",
       " 'and',\n",
       " 'lives',\n",
       " 'are',\n",
       " 'lost',\n",
       " 'at',\n",
       " 'a',\n",
       " 'very',\n",
       " 'alarming',\n",
       " 'rate',\n",
       " 'we',\n",
       " 'can',\n",
       " 'easily',\n",
       " 'gauge',\n",
       " 'our',\n",
       " 'helplessness',\n",
       " 'however',\n",
       " 'the',\n",
       " 'scenario',\n",
       " 'is',\n",
       " 'certainly',\n",
       " 'improving',\n",
       " 'now',\n",
       " 'with',\n",
       " 'the',\n",
       " 'advent',\n",
       " 'of',\n",
       " 'digitization',\n",
       " 'into',\n",
       " 'the',\n",
       " 'healthcare',\n",
       " 'system',\n",
       " 'healthcare',\n",
       " 'providers',\n",
       " 'or',\n",
       " 'practitioners',\n",
       " 'are',\n",
       " 'now',\n",
       " 'having',\n",
       " 'access',\n",
       " 'to',\n",
       " 'a',\n",
       " 'huge',\n",
       " 'amount',\n",
       " 'of',\n",
       " 'patient',\n",
       " 'health',\n",
       " 'data',\n",
       " 'this',\n",
       " 'healthcare',\n",
       " 'big',\n",
       " 'data',\n",
       " 'can',\n",
       " 'be',\n",
       " 'processed',\n",
       " 'and',\n",
       " 'analyzed',\n",
       " 'to',\n",
       " 'identify',\n",
       " 'patient',\n",
       " 'patterns',\n",
       " 'more',\n",
       " 'quickly',\n",
       " 'and',\n",
       " 'effectively',\n",
       " 'the',\n",
       " 'information',\n",
       " 'obtained',\n",
       " 'can',\n",
       " 'be',\n",
       " 'extremely',\n",
       " 'useful',\n",
       " 'to',\n",
       " 'figure',\n",
       " 'out',\n",
       " 'chronic',\n",
       " 'health',\n",
       " 'issues',\n",
       " 'and',\n",
       " 'provide',\n",
       " 'preventive',\n",
       " 'treatment',\n",
       " 'plans',\n",
       " 'well',\n",
       " 'beforehand',\n",
       " 'so',\n",
       " 'as',\n",
       " 'to',\n",
       " 'curb',\n",
       " 'that',\n",
       " 'disease',\n",
       " 'or',\n",
       " 'disorder',\n",
       " 'from',\n",
       " 'occurring',\n",
       " 'this',\n",
       " 'method',\n",
       " 'is',\n",
       " 'also',\n",
       " 'known',\n",
       " 'as',\n",
       " 'predictive',\n",
       " 'analysis',\n",
       " 'and',\n",
       " 'it',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'most',\n",
       " 'crucial',\n",
       " 'benefits',\n",
       " 'of',\n",
       " 'big',\n",
       " 'data',\n",
       " 'in',\n",
       " 'healthcare',\n",
       " 'this',\n",
       " 'technology',\n",
       " 'can',\n",
       " 'help',\n",
       " 'the',\n",
       " 'healthcare',\n",
       " 'industry',\n",
       " 'in',\n",
       " 'more',\n",
       " 'ways',\n",
       " 'than',\n",
       " 'we',\n",
       " 'can',\n",
       " 'infer',\n",
       " 'healthcare',\n",
       " 'organizations',\n",
       " 'that',\n",
       " 'have',\n",
       " 'implemented',\n",
       " 'predictive',\n",
       " 'analysis',\n",
       " 'have',\n",
       " 'witnessed',\n",
       " 'a',\n",
       " 'reduction',\n",
       " 'in',\n",
       " 'er',\n",
       " 'visits',\n",
       " 'by',\n",
       " 'providing',\n",
       " 'support',\n",
       " 'and',\n",
       " 'care',\n",
       " 'to',\n",
       " 'patients',\n",
       " 'and',\n",
       " 'decreasing',\n",
       " 'emergency',\n",
       " 'situations',\n",
       " 'apart',\n",
       " 'from',\n",
       " 'reduced',\n",
       " 'er',\n",
       " 'visits',\n",
       " 'and',\n",
       " 'timely',\n",
       " 'treatment',\n",
       " 'there',\n",
       " 'are',\n",
       " 'many',\n",
       " 'other',\n",
       " 'benefits',\n",
       " 'of',\n",
       " 'big',\n",
       " 'data',\n",
       " 'and',\n",
       " 'predictive',\n",
       " 'analysis',\n",
       " 'patients',\n",
       " 'with',\n",
       " 'highrisk',\n",
       " 'lifethreatening',\n",
       " 'issues',\n",
       " 'can',\n",
       " 'be',\n",
       " 'provided',\n",
       " 'with',\n",
       " 'more',\n",
       " 'customized',\n",
       " 'treatment',\n",
       " 'facilities',\n",
       " 'due',\n",
       " 'to',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'data',\n",
       " 'there',\n",
       " 'exists',\n",
       " 'a',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'proper',\n",
       " 'planning',\n",
       " 'in',\n",
       " 'hospitals',\n",
       " 'under',\n",
       " 'or',\n",
       " 'overbooking',\n",
       " 'of',\n",
       " 'staff',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'medical',\n",
       " 'equipment',\n",
       " 'medicines',\n",
       " 'and',\n",
       " 'other',\n",
       " 'facilities',\n",
       " 'are',\n",
       " 'a',\n",
       " 'result',\n",
       " 'of',\n",
       " 'inefficient',\n",
       " 'budgeting',\n",
       " 'and',\n",
       " 'illmanagement',\n",
       " 'of',\n",
       " 'finances',\n",
       " 'using',\n",
       " 'predictive',\n",
       " 'analysis',\n",
       " 'these',\n",
       " 'problems',\n",
       " 'can',\n",
       " 'be',\n",
       " 'solved',\n",
       " 'which',\n",
       " 'will',\n",
       " 'lead',\n",
       " 'to',\n",
       " 'reduced',\n",
       " 'costs',\n",
       " 'and',\n",
       " 'efficient',\n",
       " 'management',\n",
       " 'of',\n",
       " 'finances',\n",
       " 'better',\n",
       " 'staff',\n",
       " 'allocation',\n",
       " 'and',\n",
       " 'admission',\n",
       " 'rate',\n",
       " 'prediction',\n",
       " 'shall',\n",
       " 'facilitate',\n",
       " 'improvement',\n",
       " 'in',\n",
       " 'daily',\n",
       " 'operations',\n",
       " 'of',\n",
       " 'healthcare',\n",
       " 'organizations',\n",
       " 'by',\n",
       " 'using',\n",
       " 'big',\n",
       " 'data',\n",
       " 'the',\n",
       " 'effect',\n",
       " 'of',\n",
       " 'recency',\n",
       " 'bias',\n",
       " 'could',\n",
       " 'be',\n",
       " 'reduced',\n",
       " 'as',\n",
       " 'well',\n",
       " 'when',\n",
       " 'we',\n",
       " 'give',\n",
       " 'more',\n",
       " 'importance',\n",
       " 'to',\n",
       " 'recent',\n",
       " 'events',\n",
       " 'and',\n",
       " 'tend',\n",
       " 'to',\n",
       " 'ignore',\n",
       " 'the',\n",
       " 'effect',\n",
       " 'of',\n",
       " 'the',\n",
       " 'older',\n",
       " 'ones',\n",
       " 'it',\n",
       " 'may',\n",
       " 'lead',\n",
       " 'to',\n",
       " 'incorrect',\n",
       " 'decisions',\n",
       " 'this',\n",
       " 'is',\n",
       " 'known',\n",
       " 'as',\n",
       " 'recency',\n",
       " 'bias',\n",
       " 'big',\n",
       " 'data',\n",
       " 'also',\n",
       " 'helps',\n",
       " 'in',\n",
       " 'preventing',\n",
       " 'fraudulent',\n",
       " 'activities',\n",
       " 'which',\n",
       " 'in',\n",
       " 'turn',\n",
       " 'prevents',\n",
       " 'losses',\n",
       " 'of',\n",
       " 'insurance',\n",
       " 'companies',\n",
       " 'while',\n",
       " 'all',\n",
       " 'of',\n",
       " 'this',\n",
       " 'is',\n",
       " 'changing',\n",
       " 'the',\n",
       " 'healthcare',\n",
       " 'industry',\n",
       " 'for',\n",
       " 'the',\n",
       " 'better',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'that',\n",
       " 'easy',\n",
       " 'to',\n",
       " 'reap',\n",
       " 'the',\n",
       " 'benefits',\n",
       " 'of',\n",
       " 'big',\n",
       " 'data',\n",
       " 'there',\n",
       " 'are',\n",
       " 'a',\n",
       " 'whole',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'challenges',\n",
       " 'and',\n",
       " 'vulnerabilities',\n",
       " 'attached',\n",
       " 'to',\n",
       " 'its',\n",
       " 'implementation',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'biggest',\n",
       " 'challenges',\n",
       " 'is',\n",
       " 'security',\n",
       " 'healthcare',\n",
       " 'big',\n",
       " 'data',\n",
       " 'contains',\n",
       " 'the',\n",
       " 'personal',\n",
       " 'information',\n",
       " 'and',\n",
       " 'health',\n",
       " 'history',\n",
       " 'of',\n",
       " 'patients',\n",
       " 'acts',\n",
       " 'of',\n",
       " 'hacking',\n",
       " 'cyber',\n",
       " 'theft',\n",
       " 'and',\n",
       " 'phishing',\n",
       " 'pose',\n",
       " 'a',\n",
       " 'serious',\n",
       " 'threat',\n",
       " 'to',\n",
       " 'these',\n",
       " 'databases',\n",
       " 'such',\n",
       " 'data',\n",
       " 'could',\n",
       " 'be',\n",
       " 'stolen',\n",
       " 'and',\n",
       " 'sold',\n",
       " 'for',\n",
       " 'huge',\n",
       " 'sums',\n",
       " 'of',\n",
       " 'money',\n",
       " 'protection',\n",
       " 'of',\n",
       " 'the',\n",
       " 'patients',\n",
       " 'privacy',\n",
       " 'hence',\n",
       " 'is',\n",
       " 'a',\n",
       " 'serious',\n",
       " 'challenge',\n",
       " 'to',\n",
       " 'big',\n",
       " 'data',\n",
       " 'implementation',\n",
       " 'also',\n",
       " 'the',\n",
       " 'data',\n",
       " 'would',\n",
       " 'contain',\n",
       " 'external',\n",
       " 'data',\n",
       " 'apart',\n",
       " 'from',\n",
       " 'medical',\n",
       " 'information',\n",
       " 'the',\n",
       " 'organization',\n",
       " 'therefore',\n",
       " 'has',\n",
       " 'to',\n",
       " 'take',\n",
       " 'care',\n",
       " 'of',\n",
       " 'privacy',\n",
       " 'legal',\n",
       " 'compliances',\n",
       " 'and',\n",
       " 'government',\n",
       " 'policies',\n",
       " 'privacy',\n",
       " 'and',\n",
       " 'security',\n",
       " 'of',\n",
       " 'patients',\n",
       " 'have',\n",
       " 'to',\n",
       " 'be',\n",
       " 'given',\n",
       " 'utmost',\n",
       " 'importance',\n",
       " 'and',\n",
       " 'no',\n",
       " 'breach',\n",
       " 'of',\n",
       " 'any',\n",
       " 'kind',\n",
       " 'can',\n",
       " 'be',\n",
       " 'permitted',\n",
       " 'the',\n",
       " 'next',\n",
       " 'challenge',\n",
       " 'is',\n",
       " 'data',\n",
       " 'classification',\n",
       " 'and',\n",
       " 'modeling',\n",
       " 'the',\n",
       " 'size',\n",
       " 'of',\n",
       " 'the',\n",
       " 'data',\n",
       " 'is',\n",
       " 'massive',\n",
       " 'and',\n",
       " 'it',\n",
       " 'is',\n",
       " 'less',\n",
       " 'structured',\n",
       " 'and',\n",
       " 'heterogeneous',\n",
       " 'classifying',\n",
       " 'such',\n",
       " 'massive',\n",
       " 'data',\n",
       " 'to',\n",
       " 'identify',\n",
       " 'relevant',\n",
       " 'information',\n",
       " 'is',\n",
       " 'a',\n",
       " 'big',\n",
       " 'challenge',\n",
       " 'modeling',\n",
       " 'of',\n",
       " 'such',\n",
       " 'unstructured',\n",
       " 'data',\n",
       " 'is',\n",
       " 'equally',\n",
       " 'difficult',\n",
       " 'storage',\n",
       " 'and',\n",
       " 'retrieval',\n",
       " 'is',\n",
       " 'another',\n",
       " 'major',\n",
       " 'challenge',\n",
       " 'huge',\n",
       " 'cloud',\n",
       " 'servers',\n",
       " 'with',\n",
       " 'sufficient',\n",
       " 'space',\n",
       " 'are',\n",
       " 'required',\n",
       " 'to',\n",
       " 'store',\n",
       " 'such',\n",
       " 'voluminous',\n",
       " 'data',\n",
       " 'also',\n",
       " 'the',\n",
       " 'speed',\n",
       " 'should',\n",
       " 'be',\n",
       " 'high',\n",
       " 'so',\n",
       " 'that',\n",
       " 'uploading',\n",
       " 'of',\n",
       " 'data',\n",
       " 'can',\n",
       " 'be',\n",
       " 'done',\n",
       " 'hasslefree',\n",
       " 'the',\n",
       " 'way',\n",
       " 'storage',\n",
       " 'is',\n",
       " 'a',\n",
       " 'challenge',\n",
       " 'retrieval',\n",
       " 'also',\n",
       " 'is',\n",
       " 'a',\n",
       " 'matter',\n",
       " 'of',\n",
       " 'concern',\n",
       " 'integrating',\n",
       " 'the',\n",
       " 'data',\n",
       " 'and',\n",
       " 'getting',\n",
       " 'all',\n",
       " 'relevant',\n",
       " 'systems',\n",
       " 'to',\n",
       " 'link',\n",
       " 'each',\n",
       " 'other',\n",
       " 'is',\n",
       " 'a',\n",
       " 'tough',\n",
       " 'job',\n",
       " 'the',\n",
       " 'next',\n",
       " 'challenge',\n",
       " 'is',\n",
       " 'a',\n",
       " 'major',\n",
       " 'one',\n",
       " 'in',\n",
       " 'my',\n",
       " 'opinion',\n",
       " 'finding',\n",
       " 'the',\n",
       " 'right\\ntalent',\n",
       " 'who',\n",
       " 'own',\n",
       " 'the',\n",
       " 'expertise',\n",
       " 'to',\n",
       " 'implement',\n",
       " 'this',\n",
       " 'modern',\n",
       " 'technology',\n",
       " 'is',\n",
       " 'an',\n",
       " 'arduous\\ntask',\n",
       " 'shortage',\n",
       " 'of',\n",
       " 'required',\n",
       " 'talent',\n",
       " 'is',\n",
       " 'a',\n",
       " 'crisis',\n",
       " 'in',\n",
       " 'the',\n",
       " 'market',\n",
       " 'today',\n",
       " 'even',\n",
       " 'after\\nhiring',\n",
       " 'the',\n",
       " 'right',\n",
       " 'talent',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'challenge',\n",
       " 'to',\n",
       " 'retain',\n",
       " 'them',\n",
       " 'scarce',\n",
       " 'resources',\n",
       " 'like\\ndata',\n",
       " 'scientists',\n",
       " 'are',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'find',\n",
       " 'and',\n",
       " 'even',\n",
       " 'harder',\n",
       " 'to',\n",
       " 'retain',\n",
       " 'they',\n",
       " 'are',\n",
       " 'easily\\npoached',\n",
       " 'by',\n",
       " 'competitors',\n",
       " 'a',\n",
       " 'good',\n",
       " 'and',\n",
       " 'efficient',\n",
       " 'compensation',\n",
       " 'strategy',\n",
       " 'conducive\\nwork',\n",
       " 'environment',\n",
       " 'high',\n",
       " 'incentives',\n",
       " 'opportunities',\n",
       " 'for',\n",
       " 'career',\n",
       " 'growth',\n",
       " 'and',\n",
       " 'development\\ncan',\n",
       " 'be',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'ways',\n",
       " 'of',\n",
       " 'retaining',\n",
       " 'such',\n",
       " 'intellectual',\n",
       " 'talent',\n",
       " 'in',\n",
       " 'a',\n",
       " 'nutshell',\n",
       " 'we',\n",
       " 'can',\n",
       " 'conclude',\n",
       " 'that',\n",
       " 'while',\n",
       " 'big',\n",
       " 'data',\n",
       " 'is',\n",
       " 'a',\n",
       " 'disruptive',\n",
       " 'technology',\n",
       " ...]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "21612f05",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.0</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.0</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.0</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146.0</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>147.0</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>148.0</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>149.0</td>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>150.0</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL\n",
       "0      37.0  https://insights.blackcoffer.com/ai-in-healthc...\n",
       "1      38.0  https://insights.blackcoffer.com/what-if-the-c...\n",
       "2      39.0  https://insights.blackcoffer.com/what-jobs-wil...\n",
       "3      40.0  https://insights.blackcoffer.com/will-machine-...\n",
       "4      41.0  https://insights.blackcoffer.com/will-ai-repla...\n",
       "..      ...                                                ...\n",
       "109   146.0  https://insights.blackcoffer.com/blockchain-fo...\n",
       "110   147.0  https://insights.blackcoffer.com/the-future-of...\n",
       "111   148.0  https://insights.blackcoffer.com/big-data-anal...\n",
       "112   149.0  https://insights.blackcoffer.com/business-anal...\n",
       "113   150.0  https://insights.blackcoffer.com/challenges-an...\n",
       "\n",
       "[114 rows x 2 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0c4bc015",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.concat([input, df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9884b74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.0</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>0.098739</td>\n",
       "      <td>23.760000</td>\n",
       "      <td>0.327722</td>\n",
       "      <td>9.635089</td>\n",
       "      <td>23.760000</td>\n",
       "      <td>584.0</td>\n",
       "      <td>1782.0</td>\n",
       "      <td>2.107183</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.670595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>58.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.221053</td>\n",
       "      <td>0.174312</td>\n",
       "      <td>17.625000</td>\n",
       "      <td>0.208511</td>\n",
       "      <td>7.133404</td>\n",
       "      <td>17.625000</td>\n",
       "      <td>294.0</td>\n",
       "      <td>1410.0</td>\n",
       "      <td>1.801418</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.842553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.0</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.292929</td>\n",
       "      <td>0.123441</td>\n",
       "      <td>19.952941</td>\n",
       "      <td>0.300708</td>\n",
       "      <td>8.101459</td>\n",
       "      <td>19.952941</td>\n",
       "      <td>510.0</td>\n",
       "      <td>1696.0</td>\n",
       "      <td>2.020637</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.400943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.0</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>59.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.140753</td>\n",
       "      <td>17.391304</td>\n",
       "      <td>0.214375</td>\n",
       "      <td>7.042272</td>\n",
       "      <td>17.391304</td>\n",
       "      <td>343.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1.838125</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.805625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>53.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.102426</td>\n",
       "      <td>22.077922</td>\n",
       "      <td>0.231176</td>\n",
       "      <td>8.923639</td>\n",
       "      <td>22.077922</td>\n",
       "      <td>393.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1.860000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.078824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146.0</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>147.0</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>148.0</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>149.0</td>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>150.0</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL  \\\n",
       "0      37.0  https://insights.blackcoffer.com/ai-in-healthc...   \n",
       "1      38.0  https://insights.blackcoffer.com/what-if-the-c...   \n",
       "2      39.0  https://insights.blackcoffer.com/what-jobs-wil...   \n",
       "3      40.0  https://insights.blackcoffer.com/will-machine-...   \n",
       "4      41.0  https://insights.blackcoffer.com/will-ai-repla...   \n",
       "..      ...                                                ...   \n",
       "109   146.0  https://insights.blackcoffer.com/blockchain-fo...   \n",
       "110   147.0  https://insights.blackcoffer.com/the-future-of...   \n",
       "111   148.0  https://insights.blackcoffer.com/big-data-anal...   \n",
       "112   149.0  https://insights.blackcoffer.com/business-anal...   \n",
       "113   150.0  https://insights.blackcoffer.com/challenges-an...   \n",
       "\n",
       "     POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0              62.0            32.0        0.319149            0.098739   \n",
       "1              58.0            37.0        0.221053            0.174312   \n",
       "2              64.0            35.0        0.292929            0.123441   \n",
       "3              59.0            27.0        0.372093            0.140753   \n",
       "4              53.0            23.0        0.394737            0.102426   \n",
       "..              ...             ...             ...                 ...   \n",
       "109             NaN             NaN             NaN                 NaN   \n",
       "110             NaN             NaN             NaN                 NaN   \n",
       "111             NaN             NaN             NaN                 NaN   \n",
       "112             NaN             NaN             NaN                 NaN   \n",
       "113             NaN             NaN             NaN                 NaN   \n",
       "\n",
       "     AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0              23.760000                     0.327722   9.635089   \n",
       "1              17.625000                     0.208511   7.133404   \n",
       "2              19.952941                     0.300708   8.101459   \n",
       "3              17.391304                     0.214375   7.042272   \n",
       "4              22.077922                     0.231176   8.923639   \n",
       "..                   ...                          ...        ...   \n",
       "109                  NaN                          NaN        NaN   \n",
       "110                  NaN                          NaN        NaN   \n",
       "111                  NaN                          NaN        NaN   \n",
       "112                  NaN                          NaN        NaN   \n",
       "113                  NaN                          NaN        NaN   \n",
       "\n",
       "     AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                           23.760000               584.0      1782.0   \n",
       "1                           17.625000               294.0      1410.0   \n",
       "2                           19.952941               510.0      1696.0   \n",
       "3                           17.391304               343.0      1600.0   \n",
       "4                           22.077922               393.0      1700.0   \n",
       "..                                ...                 ...         ...   \n",
       "109                               NaN                 NaN         NaN   \n",
       "110                               NaN                 NaN         NaN   \n",
       "111                               NaN                 NaN         NaN   \n",
       "112                               NaN                 NaN         NaN   \n",
       "113                               NaN                 NaN         NaN   \n",
       "\n",
       "     SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0             2.107183                4.0         5.670595  \n",
       "1             1.801418                8.0         4.842553  \n",
       "2             2.020637                3.0         5.400943  \n",
       "3             1.838125               22.0         4.805625  \n",
       "4             1.860000               20.0         5.078824  \n",
       "..                 ...                ...              ...  \n",
       "109                NaN                NaN              NaN  \n",
       "110                NaN                NaN              NaN  \n",
       "111                NaN                NaN              NaN  \n",
       "112                NaN                NaN              NaN  \n",
       "113                NaN                NaN              NaN  \n",
       "\n",
       "[114 rows x 15 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "db839c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('C:/Users/91942/Desktop/Blackcoffer/Data/Output Data Structure.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "651a42ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_excel('C:/Users/91942/Desktop/Blackcoffer/Data/Output Data Structure.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
